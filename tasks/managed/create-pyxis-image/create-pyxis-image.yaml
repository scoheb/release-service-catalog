---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: create-pyxis-image
  annotations:
    tekton.dev/pipelines.minVersion: "0.12.1"
    tekton.dev/tags: release
spec:
  description: |-
    Tekton task that pushes metadata to Pyxis for all container images contained in a snapshot that is the
    result of the `apply-mapping` task. It first extracts the containerImages from the snapshot, then runs
    `skopeo inspect` on each, before finally pushing metadata to Pyxis.
    
    The task includes protection against race conditions that could occur when multiple components have the same
    digest. Components with the same digest are processed in series while different digest groups are processed
    in parallel, ensuring optimal performance while preventing conflicts.
    
    The relative path of the pyxis.json file in the data workspace is output as a task result named
    `pyxisDataPath`.
  params:
    - name: server
      type: string
      description: The server type to use. Options are 'production','production-internal,'stage-internal' and 'stage'
      default: production
    - name: concurrentLimit
      type: string
      description: The maximum number of components to be processed at once
      default: "16"
    - name: pyxisSecret
      type: string
      description: |
        The kubernetes secret to use to authenticate to Pyxis. It needs to contain two keys: key and cert
    - name: certified
      type: string
      description: If set to true, the images will be marked as certified in their Pyxis entries
      default: "false"
    - name: isLatest
      type: string
      description: If set to true, the images will have a latest tag added with their Pyxis entries
      default: "false"
    - name: rhPush
      type: string
      description: >
        If set to true, an additional entry will be created in ContainerImage.repositories with
        the registry and repository fields converted to use Red Hat's official registry.
        E.g. a mapped repository of "quay.io/redhat-pending/product---my-image" will be converted
        to use registry "registry.access.redhat.com" and repository "product/my-image". Also, this
        repository entry will be marked as published
      default: "false"
    - name: snapshotPath
      type: string
      description: Path to the JSON string of the mapped Snapshot spec in the data workspace
    - name: dataPath
      description: Path to the JSON string of the merged data to use in the data workspace
      type: string
    - name: ociStorage
      description: The OCI repository where the Trusted Artifacts are stored
      type: string
      default: "empty"
    - name: ociArtifactExpiresAfter
      description: Expiration date for the trusted artifacts created in the
        OCI repository. An empty string means the artifacts do not expire
      type: string
      default: "1d"
    - name: trustedArtifactsDebug
      description: Flag to enable debug logging in trusted artifacts. Set to a non-empty string to enable
      type: string
      default: ""
    - name: orasOptions
      description: oras options to pass to Trusted Artifacts calls
      type: string
      default: ""
    - name: sourceDataArtifact
      type: string
      description: Location of trusted artifacts to be used to populate data directory
      default: ""
    - name: dataDir
      description: The location where data will be stored
      type: string
      default: /var/workdir/release
    - name: taskGitUrl
      type: string
      description: The url to the git repo where the release-service-catalog tasks and stepactions to be used are stored
    - name: taskGitRevision
      type: string
      description: The revision in the taskGitUrl repo to be used
  results:
    - name: pyxisDataPath
      description: The relative path in the workspace to the stored pyxis data json
    - description: Produced trusted data artifact
      name: sourceDataArtifact
      type: string
  volumes:
    - name: workdir
      emptyDir: {}
  stepTemplate:
    volumeMounts:
      - mountPath: /var/workdir
        name: workdir
    env:
      - name: IMAGE_EXPIRES_AFTER
        value: $(params.ociArtifactExpiresAfter)
      - name: "ORAS_OPTIONS"
        value: "$(params.orasOptions)"
      - name: "DEBUG"
        value: "$(params.trustedArtifactsDebug)"
  steps:
    - name: use-trusted-artifact
      computeResources:
        limits:
          memory: 64Mi
        requests:
          memory: 64Mi
          cpu: 30m
      ref:
        resolver: "git"
        params:
          - name: url
            value: $(params.taskGitUrl)
          - name: revision
            value: $(params.taskGitRevision)
          - name: pathInRepo
            value: stepactions/use-trusted-artifact/use-trusted-artifact.yaml
      params:
        - name: workDir
          value: $(params.dataDir)
        - name: sourceDataArtifact
          value: $(params.sourceDataArtifact)
    - name: create-pyxis-image
      image: quay.io/konflux-ci/release-service-utils:ea7868ebdcc7a2116c620255034226611d837f42
      computeResources:
        limits:
          memory: 3Gi
        requests:
          memory: 3Gi
          cpu: '1'
      env:
        - name: pyxisCert
          valueFrom:
            secretKeyRef:
              name: $(params.pyxisSecret)
              key: cert
        - name: pyxisKey
          valueFrom:
            secretKeyRef:
              name: $(params.pyxisSecret)
              key: key
      script: |
        #!/usr/bin/env bash
        set -exo pipefail

        if [[ "$(params.server)" == "production" ]]
        then
          PYXIS_URL="https://pyxis.api.redhat.com/"
          PYXIS_GRAPHQL_URL="https://graphql-pyxis.api.redhat.com/graphql/"
        elif [[ "$(params.server)" == "stage" ]]
        then
          PYXIS_URL="https://pyxis.preprod.api.redhat.com/"
          PYXIS_GRAPHQL_URL="https://graphql-pyxis.preprod.api.redhat.com/graphql/"
        elif [[ "$(params.server)" == "production-internal" ]]
        then
          PYXIS_URL="https://pyxis.engineering.redhat.com/"
          PYXIS_GRAPHQL_URL="https://graphql.pyxis.engineering.redhat.com/graphql/"
        elif [[ "$(params.server)" == "stage-internal" ]]
        then
          PYXIS_URL="https://pyxis.stage.engineering.redhat.com/"
          PYXIS_GRAPHQL_URL="https://graphql.pyxis.stage.engineering.redhat.com/graphql/"
        else
          echo "Invalid server parameter. Only 'production','production-internal,'stage-internal' and 'stage' allowed."
          exit 1
        fi

        SNAPSHOT_SPEC_FILE="$(params.dataDir)/$(params.snapshotPath)"
        if [ ! -f "${SNAPSHOT_SPEC_FILE}" ] ; then
            echo "No valid snapshot file was provided."
            exit 1
        fi

        DATA_FILE="$(params.dataDir)/$(params.dataPath)"
        if [ ! -f "${DATA_FILE}" ] ; then
            echo "No data JSON was provided."
            exit 1
        fi

        PYXIS_DATA_PATH="$(dirname "$(params.snapshotPath)")/pyxis.json"
        echo -n "${PYXIS_DATA_PATH}" > "$(results.pyxisDataPath.path)"

        set +x
        echo "${pyxisCert:?}" > /tmp/crt
        echo "${pyxisKey:?}" > /tmp/key
        set -x

        AUTH_FILE=$(mktemp)

        # Default to false
        includeLayers="$(jq -r ".pyxis.includeLayers // false" "${DATA_FILE}")"

        # Function to process a single component in parallel
        process_component() {
            local component_index=$1
            local output_file="/tmp/component_${component_index}.json"

            CONTAINER_IMAGE=$(jq -r ".components[${component_index}].containerImage" "${SNAPSHOT_SPEC_FILE}")
            REPOSITORY=$(jq -r ".components[${component_index}].repository" "${SNAPSHOT_SPEC_FILE}")
            REPOSITORY=${REPOSITORY%:*} # strip tag just in case - it should not be there
            SOURCE_REPO=${CONTAINER_IMAGE%%@sha256:*}
            DIGEST="${CONTAINER_IMAGE##*@}"
            PULLSPEC="${REPOSITORY}@${DIGEST}"
            MEDIA_TYPE=$(skopeo inspect --retry-times 3 --raw "docker://${PULLSPEC}" | jq -r .mediaType)
            TAGS=$(jq -r ".components[${component_index}].tags | join(\" \")" "${SNAPSHOT_SPEC_FILE}")

            # Create component-specific auth file to avoid conflicts
            local AUTH_FILE="/tmp/auth_${component_index}"

            # oras has very limited support for selecting the right auth entry,
            # so create a custom auth file with just one entry before each oras call
            select-oci-auth "${SOURCE_REPO}" > "$AUTH_FILE"
            DOCKERFILE_DIR="$(mktemp -d)"
            DOCKERFILE_PATH=""
            DOCKERFILE_PULL_SPEC="${SOURCE_REPO}:${DIGEST/:/-}.dockerfile"
            # try fetching Dockerfile for the image
            if oras pull --registry-config "$AUTH_FILE" "${DOCKERFILE_PULL_SPEC}" -o "${DOCKERFILE_DIR}"
            then
              DOCKERFILE_PATH="${DOCKERFILE_DIR}/Dockerfile"
              if [ ! -f "${DOCKERFILE_PATH}" ]; then
                echo Error: Dockerfile pull succeeded, but the Dockerfile was not saved.
                exit 1
              fi
            else
              echo "Unable to get Dockerfile for the image. Maybe it's not enabled in the build pipeline?"
            fi

            select-oci-auth "${REPOSITORY}" > "$AUTH_FILE"

            ARCHITECTURES=$(get-image-architectures "${PULLSPEC}")

            # Initialize component JSON structure
            local COMPONENT_JSON='{}'
            COMPONENT_JSON=$(jq --argjson id "$component_index" --arg image "${CONTAINER_IMAGE}" \
              '.containerImage = $image | .componentIndex = $id | .pyxisImages = []' <<< "$COMPONENT_JSON")

            index=0
            while IFS= read -r ARCH_DETAIL;
            do
                OS=$(jq -r '.platform.os' <<< "$ARCH_DETAIL")
                ARCH=$(jq -r '.platform.architecture' <<< "$ARCH_DETAIL")
                ARCH_DIGEST=$(jq -r '.digest' <<< "$ARCH_DETAIL")

                ORAS_ARGS=()
                if [[ "$MEDIA_TYPE" == "application/vnd.docker.distribution.manifest.list.v2+json" ]]\
                  || [[ "$MEDIA_TYPE" == "application/vnd.oci.image.index.v1+json" ]]; then
                  ORAS_ARGS+=(--platform "$OS/$ARCH")
                fi

                # Save the OCI manifest locally, to pass to a script to create the pyxis entry
                # Use component-specific manifest file to avoid conflicts
                MANIFEST_FILE="$(params.dataDir)/$(dirname \
                  "$(params.snapshotPath)")/oras-manifest-fetch-${component_index}-${index}.json"
                oras manifest fetch \
                  --registry-config "$AUTH_FILE" \
                  "${ORAS_ARGS[@]}" \
                  "${PULLSPEC}" \
                      | tee "${MANIFEST_FILE}"

                # When building images without squashing, their final layer might always be bit-wise identical.
                # This causes upload to pyxis to fail which cannot tolerate duplicate "top_layer_id" values.
                # This flag allows to overcome this limitation by just deleting the layers so that no layer
                # information is uploaded.
                if [ "$includeLayers" != true ]; then
                  echo ".pyxis.includeLayers is not true in data file, so delete the layers"
                  jq '.layers = []' "${MANIFEST_FILE}" > "${MANIFEST_FILE}.tmp"
                  mv "${MANIFEST_FILE}.tmp"  "${MANIFEST_FILE}"
                fi

                # Augment that manifest with further information about the layers, decompressed
                # This requires pulling the layers to decompress and then measure them
                while IFS= read -r BLOB_DETAIL;
                do
                    BLOB_TYPE=$(jq -r '.mediaType' <<< "$BLOB_DETAIL")
                    BLOB_DIGEST=$(jq -r '.digest' <<< "$BLOB_DETAIL")

                    # Normal images will always have the layers compressed.
                    # If they are not compressed, this will not save the
                    # uncompressed data in Pyxis.
                    #
                    # It's also possible that the layers are compressed with
                    # some scheme other than gzip.  In that case, the
                    # uncompressed layer information will also not be saved in
                    # Pyxis.
                    # https://github.com/konflux-ci/build-definitions/issues/1264

                    if [[ "$BLOB_TYPE" =~ ^.*\.gzip$|^.*\+gzip$ ]]; then
                        BLOB_FILE="/tmp/oras-blob-fetch-${component_index}-${BLOB_DIGEST}"
                        BLOB_PULLSPEC="${PULLSPEC%%@*}@${BLOB_DIGEST}"

                        # Save the blob
                        oras blob fetch \
                          --registry-config "$AUTH_FILE" \
                          --output "${BLOB_FILE}.gz" \
                          "${BLOB_PULLSPEC}"

                        # Decompress it
                        gunzip "${BLOB_FILE}.gz"

                        # Measure it
                        EXPANDED_DIGEST="sha256:$(sha256sum "${BLOB_FILE}" | cut -d " " -f 1)"
                        EXPANDED_SIZE=$(wc --bytes "${BLOB_FILE}" | awk '{print $1}' | tr -d '\n')

                        # Append this information to the parsed_data manifest
                        jq \
                          '.uncompressed_layers += [{"digest": "'"$EXPANDED_DIGEST"'", "size": '"$EXPANDED_SIZE"'}]' \
                          "${MANIFEST_FILE}" > "${MANIFEST_FILE}.tmp"
                        mv "${MANIFEST_FILE}.tmp"  "${MANIFEST_FILE}"

                        # Clean up, in case we're dealing with large images
                        rm "${BLOB_FILE}"
                    fi
                done <<< "$(jq -c '.layers[]' "${MANIFEST_FILE}")"

                PYXIS_CERT_PATH=/tmp/crt PYXIS_KEY_PATH=/tmp/key create_container_image \
                  --pyxis-url $PYXIS_URL \
                  --certified "$(params.certified)" \
                  --tags "$TAGS" \
                  --is-latest "$(params.isLatest)" \
                  --verbose \
                  --oras-manifest-fetch "${MANIFEST_FILE}" \
                  --name "$REPOSITORY" \
                  --media-type "$MEDIA_TYPE" \
                  --digest "$DIGEST" \
                  --architecture-digest "$ARCH_DIGEST" \
                  --architecture "$ARCH" \
                  --rh-push "$(params.rhPush)" \
                  --dockerfile "${DOCKERFILE_PATH}" | tee "/tmp/output_${component_index}_${index}"
                # The rh-push-to-external-registry e2e test depends on this line being in the task log
                IMAGEID=$(awk '/The image id is/{print $NF}' "/tmp/output_${component_index}_${index}" | head -1)

                # prepare the REPOSITORY string for cleanup_tags
                TARGET_REPOSITORY="${REPOSITORY#*/}" # remove the host
                TARGET_REPOSITORY="${TARGET_REPOSITORY//----/\/}"
                TARGET_REPOSITORY="${TARGET_REPOSITORY#*/}"

                # Remove the new image tags from all previous images, but only if rhPush=true
                if [ "$(params.rhPush)" = "true" ]; then
                  PYXIS_CERT_PATH=/tmp/crt PYXIS_KEY_PATH=/tmp/key cleanup_tags \
                    --verbose \
                    --retry \
                    --pyxis-graphql-api $PYXIS_GRAPHQL_URL \
                    --repository "$TARGET_REPOSITORY" \
                    "$IMAGEID"
                fi

                COMPONENT_JSON=$(jq --argjson arch_index $index \
                  --arg arch "${ARCH}" --arg imageId "${IMAGEID}" --arg digest "${DIGEST}" \
                  --arg arch_digest "${ARCH_DIGEST}" --arg os "${OS}" \
                    '.pyxisImages[$arch_index] += {
                      "arch": $arch,
                      "imageId": $imageId,
                      "digest": $digest,
                      "arch_digest": $arch_digest,
                      "os": $os}' <<< "$COMPONENT_JSON")

                index=$((index + 1))
            done <<< "$ARCHITECTURES"

            # Save component result to file
            echo "$COMPONENT_JSON" | tee "$output_file"
        }

        # Function to process a group of components with the same digest in series
        process_digest_group() {
            local digest=$1
            local component_indices_string="$2"
            
            echo "Processing components with digest: $digest"
            
            # Process each component in the group in series
            for component_index in $component_indices_string; do
                echo "Processing component $component_index for digest $digest"
                process_component "$component_index"
            done
        }

        COMPONENTS=$(jq '.components | length' "${SNAPSHOT_SPEC_FILE}")

        # Group components by digest to prevent race conditions
        declare -A digest_groups
        for (( i=0; i < COMPONENTS; i++ )); do
            CONTAINER_IMAGE=$(jq -r ".components[${i}].containerImage" "${SNAPSHOT_SPEC_FILE}")
            DIGEST="${CONTAINER_IMAGE##*@}"
            
            if [[ -z "${digest_groups[$DIGEST]}" ]]; then
                digest_groups[$DIGEST]="$i"
            else
                digest_groups[$DIGEST]="${digest_groups[$DIGEST]} $i"
            fi
        done

        RUNNING_JOBS="\j" # Bash parameter for number of jobs currently running
        SUCCESS=true

        # Process each digest group in parallel, but components within each group in series
        for digest in "${!digest_groups[@]}"; do
            echo "Starting parallel processing for components with digest: $digest"
            # Limit batch size to concurrent limit
            while (( ${RUNNING_JOBS@P} >= $(params.concurrentLimit) )); do
                wait -n || SUCCESS=false
            done
            # Use hex digest without sha256: prefix for filename consistency
            digest_filename="${digest#sha256:}"
            process_digest_group "$digest" "${digest_groups[$digest]}" \
              > "/tmp/component_group_${digest_filename}.out" \
              2> "/tmp/component_group_${digest_filename}.err" &
        done

        # Wait for remaining processes to finish
        while (( ${RUNNING_JOBS@P} > 0 )); do
            wait -n || SUCCESS=false
        done
        echo "All component processing jobs completed"

        # Print outputs and errors for each component processing job
        for digest in "${!digest_groups[@]}"; do
            digest_filename="${digest#sha256:}"
            if [ -f "/tmp/component_group_${digest_filename}.out" ]; then
                echo "=== Component Processing Output ==="
                cat "/tmp/component_group_${digest_filename}.out"
                echo
            fi
            if [ -f "/tmp/component_group_${digest_filename}.err" ] && \
               [ -s "/tmp/component_group_${digest_filename}.err" ]; then
                echo "=== Component Processing Errors ==="
                cat "/tmp/component_group_${digest_filename}.err"
                echo
            fi
        done

        if [ "$SUCCESS" != true ]; then
            echo "One or more component processing jobs failed. Please check the logs above for details."
            exit 1
        fi

        # Collect and merge all component results
        JSON_OUTPUT='{}'
        for (( i=0; i < COMPONENTS; i++ )); do
            if [ -f "/tmp/component_${i}.json" ]; then
                component_data=$(cat "/tmp/component_${i}.json")
                component_index=$(jq -r '.componentIndex' <<< "$component_data")
                JSON_OUTPUT=$(jq --argjson component_index "$component_index" --argjson \
                  component_data "$component_data" \
                  '.components[$component_index] = $component_data' <<< "$JSON_OUTPUT")
            fi
        done
        echo "$JSON_OUTPUT" | tee "$(params.dataDir)/${PYXIS_DATA_PATH}"
    - name: create-trusted-artifact
      computeResources:
        limits:
          memory: 128Mi
        requests:
          memory: 128Mi
          cpu: 250m
      ref:
        resolver: "git"
        params:
          - name: url
            value: "$(params.taskGitUrl)"
          - name: revision
            value: "$(params.taskGitRevision)"
          - name: pathInRepo
            value: stepactions/create-trusted-artifact/create-trusted-artifact.yaml
      params:
        - name: ociStorage
          value: $(params.ociStorage)
        - name: workDir
          value: $(params.dataDir)
        - name: sourceDataArtifact
          value: $(results.sourceDataArtifact.path)
